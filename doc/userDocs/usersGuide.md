![Weathervane](../images/VMW-Weathervane-Logo-SML.png)
# Weathervane 2.0 User's Guide
Contents:
- [Introduction](#intro)
- [QuickStart Guide](#quickstart-guide)
    - [Prerequisites](#quickstart-prereq)
- [Detailed Setup](#setup)
    - [Configuring Kubernetes Cluster(s)](#configuring-clusters)
    - [Configuring a Client](#configuring-client)
- [Using Weathervane](#using)
- [Configuration File Details](#config)
- [Run Output Details](#output)
- [Troubleshooting](#troubleshooting)
- [Advanced Topics](#advanced)

## Introduction<a name="intro"></a>

### Overview

Weathervane 2.0 is an application-level performance benchmark which lets you
investigate the performance characteristics of on-premise and cloud-based
Kubernetes clusters.

Weathervane tests the performance capabilities of a Kubernetes cluster by
deploying one or more applications on the cluster and then driving a load
against those applications.  The load is generated by the Weathervane workload
driver, which also runs on a Kubernetes cluster. Weathervane can be configured
to generate a steady load using a fixed number of simulated users, or to
automatically vary the number of users to find the maximum number that can be
supported on the cluster without violating quality-of-service (QoS)
requirements.

When using a fixed load, Weathervane will give a pass/fail result which
indicates whether the run completed without violating the QoS requirements.
When using the maximum finding feature, the result will be the largest number of
simulated users that could interact with the applications without violating the
QoS requirements.  This number is referred to as the peak WvUsers.

The application used by Weathervane is a multi-tier web application which
includes both stateless and stateful services. You can select from multiple
pre-tuned and tested configurations of this application.  The configurations
represent a range of deployment sizes. This allows you to select a configuration
based on the size of the cluster under test, or based on the expected usage of
the cluster.  Weathervane 2.0 includes two configuration sizes, and larger
configurations will be included in future releases. More details about the
application are included in the [appendix](#architecture).

Weathervane also includes a run harness that automates the process of executing
runs and collecting results.  The run harness can run on any client system that
supports Docker containers and that has connectivity to the Kubernetes clusters.

### Quality-of-Service Requirements

Weathervane enforces three types of QoS requirements on the operations performed
by the simulated users.  These QoS requirements are tested over a five minute
measurement period. The requirements are:
- Response-Time: 99% of all operations in each operation type must 
complete within that operation's response-time limit.
- Operation Mix: Each operation type must make up a predefined proportion of 
the overall operation mix. 
- Failure Percentage: Less than 0.5% of operations can experience a functional
failure.

### Performance Metrics

The primary metric from Weathervane is WvUsers.  This metric represents the
maximum number of simulated users that could interact with the application
instances without violating the QoS requirements. When running the same
deployment configuration on two different Kubernetes clusters, the cluster that
supports a higher WvUsers can be considered to be the higher performing cluster.

Weathervane also reports the average response-time for user operations.  For
systems running at identical WvUsers, the average response-time can be used to
compare the performance of different clusters.

Weathervane also reports throughput in terms of operations/second and
HTTP-requests/second, but these metrics do not add additional information over
WvUsers. As long as the QoS requirements are not violated, each WvUser drives a
fixed amount of throughput in terms of both operations-per-second and
HTTP-requests/sec.  As a result, there is a direct correlation between WvUsers
and throughput.

## Quickstart Guide<a name="quickstart-guide"></a>

This section is intended to get users up and running using Weathervane 2.0 with a minimal set of instructions.
More detailed [instructions](#setup) are included in later sections.

### Prerequisites<a name="quickstart-prereq"></a>

Weathervane requires at least one existing Kubernetes cluster and a client system.
- Kubernetes cluster:
    - You must have a kubeconfig file with [credentials](#credentials) for the cluster.
    - The cluster must support either NodePort or LoadBalancer [services](#loadbalancer).
    - There must be at least one [StorageClass](#storageclass) defined on the cluster.
- Client system:
    - Configure a workstation (MacOS or Linux) or a Linux VM with the required software:
        - Perl 5 (https://www.perl.org/get.html)
        - Docker Engine (https://docs.docker.com/install/)
        - Git (https://git-scm.com/downloads)
        - The kubeconfig file must be accessible by this client system.

Most uses of Weathervane might [use](#configuring-clusters) multiple clusters or node labels.

### Quickstart Setup

#### Obtain Weathervane from GitHub

1. Clone the Weathervane repository to the client system:
    * `git clone https://github.com/vmware/weathervane`
    * This will create a directory called weathervane in the current directory.
1. Change into the weathervane directory:
      - `cd weathervane`

    
#### Create the Weathervane Docker Images

Using the included `buildDockerImages.pl` script, build the Docker images for the Weathervane components 
and place them on a Docker registry that is accessible from your Kubernetes clusters.

The process using a Docker Hub account is as follows:
- Make sure you are in the weathervane directory and run the script:
    * `./buildDockerImages.pl --username yourUserName`
        * Replace *yourUserName* with your Docker Hub username.
        * The script will prompt for your Docker Hub password.
        * This process can take as long as an hour to complete.

Additional details for this script are documented [here](#building), including instructions to enable the use of a private Docker repository.


#### Create the Weathervane Configuration File<a name="quickstart-config"></a>
Create a file to contain your unique configuration settings by first duplicating an included config file in the weathervane directory:
* `cp weathervane.config.k8s.micro weathervane.config.k8s.quickstart`

This example file runs a very small configuration of the Weathervane application using a fixed runStrategy.
It will attempt a run with 200 users that will take about 45 minutes to complete, including about 15 minutes to load the data -- a process that should only occur on the first run.

The image below shows the lines that need to be edited in the configuration file:

![config file image](../images/wv-micro-config-example.png)
1. Replace `yourRepository` with your Docker Hub username or the hostname and port of your private Docker registry.
2. Update *kubeconfigFile* to the full path to your kubeconfig file.  The paths will be identical if using a single cluster for both the app and driver.
3. Update *kubeconfigContext* parameters with the name of the context for your cluster from the kubeconfig file, such as `kubernetes-admin@kubernetes`.
    - Set an empty string (`""`) to use the current-context defined in the kubeconfigFile.
4. Set *useLoadBalancer* to `true` if your cluster supports LoadBalancer services. Otherwise, set it to `false` to use a NodePort for ingress.
5. Update *StorageClass* parameters with the names of one or more storage classes defined on your cluster.

Notes:
- If your platform does not support creating namespaces using kubectl, then you must create the namespaces manually before the run. The workload drivers run in namespace `auctionw1`. The application runs in namespace `auctionw1i1`.

More details on the configuration file are available [here](#config).

### Quickstart Run

To run Weathervane, issue the following command in the weathervane directory and use the configuration file created above:
* `./runWeathervane.pl --configFile=weathervane.config.k8s.quickstart`

When you invoke this script for the first time, you will be prompted to accept the license terms for Weathervane.

### Examine the Output

As Weathervane runs it will print progress messages to the screen. 
Once the run completes, all output files will be copied to a sub-directory under *weathervane/output*.
More details about the output are located [here](#output).

The primary result from Weathervane run with the fixed runStrategy is a pass/fail decision for the selected number of users.

### Cleaning Up

In order to avoid reloading the data for every run,
Weathervane leaves certain data in place on the persistent storage configured with the *StorageClass* parameters.
Once the data is no longer needed, these should be properly [cleaned](#cleaning-up-persistent-storage) up.

### Next Steps

This is the end of the quickstart section. More details for [setup](#setup) and [using](#using) Weathervane are located in the following sections.

## Detailed Setup Instructions<a name="setup"></a>

### Prerequisites<a name="prereq"></a>

For a summary of prerequisites, refer to the [prerequisites](#quickstart-prereq) section in the Quickstart Guide .

### Configuring Kubernetes Cluster(s) for Weathervane<a name="configuring-clusters"></a>

#### Overview

The Weathervane 2.0 application and driver components require at least one Kubernetes cluster to run on.

For most uses of Weathervane 2.0, the workload driver pods should run on different compute resources than the application pods.
Depending on your available resources and the goals of your tests, this may be done in two ways:
1. By partitioning the worker nodes of a single cluster into driver and system-under-test 
   (SUT) nodes using [nodeLabels](#nodeLabels).
2. By using two Kubernetes clusters with the config file parameters driverCluster and appCluster.

#### Kubernetes Cluster Credentials<a name="credentials"></a>

You must have kubeconfig file(s) with credentials for the cluster(s).
- If you are running on a cloud-based Kubernetes cluster, see the documentation from your cluster provider for instructions on retrieving the credentials. Typically the credentials will end up in the file ~/.kube/config.
- If you are running on a Kubernetes cluster created for you, you may need to contact the cluster administrator for the cluster credentials.
- If you configured your own cluster using kubeadm, the credentials will be in the file /etc/kubernetes/admin.conf.

In all cases you will need to copy the credentials file to your client system.

Handling clusters whose credential expire is discussed [below](#expire).

When using both a driverCluster and appCluster, you can use separate kubeconfig files for each cluster,
or use one file with multiple contexts. More information about using multiple contexts in a kubeconfig file is located at (https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/).

#### NodePort and LoadBalancer Services<a name="loadbalancer"></a>

The application instances can use either a NodePort or LoadBalancer service to enable incoming communication from the workload driver pods (https://kubernetes.io/docs/concepts/services-networking/service/).
Most clusters will support at least one of these ingress methods, and some may support both.
You will need to check the documentation for your cluster to determine which is supported.

Using a LoadBalancer service may be more performant than a NodePort, and is more likely to match the way the cluster would be used in a production environment.
As a result, if your cluster supports both types of services, we recommend using LoadBalancer services by setting *useLoadBalancer* to `true` in your configuration file.
Please note that depending on your cluster provider there may be a cost associated with LoadBalancer services. Check with your provider for more details.


#### Configuring Persistent Storage<a name="storageclass"></a>

The Weathervane Auction application requires data be loaded into the data
services before a run can be performed. This data includes user accounts,
auction and item data, item images, and historical event data regarding bids
and auction attendance.

There must be at least one [StorageClass](#storageclass) defined on the application cluster to be used for this persistent storage.
Examples for different storage providers are available at this link (https://kubernetes.io/docs/concepts/storage/storage-classes/).

In the Weathervane config file, there are multiple *StorageClass* parameters that should be updated with names defined on your cluster.
All services may use the same StorageClass, or you can use a different StorageClass for each service.

Size requirements per instances are noted in the configuration size [tables](#configuration-sizes).

Loading the data can be time consuming, and is done at the beginning of a run if not already completed.

Here are some time estimates for loading a single application instance:
- A micro configuration with 3,000 users takes about 15 minutes.
- A small configuration with 20,000 users takes close to an hour.

##### Cleaning Up Persistent Storage<a name="cleaning-up-persistent-storage"></a>

In order to avoid reloading the data for every run, Weathervane leaves this data in place on the persistent storage.
On subsequent runs, Weathervane reuses the loaded data by restoring it to the original state.

Once the data is no longer needed, such as when no more Weathervane runs will be performed, the persistent storage should be properly cleaned up,
even if you plan to delete the clusters under test.

The command below will delete all of the PersistentVolumeClaims created by Weathervane:
* `kubectl delete pvc --selector=app=auction --all-namespaces`


### Configuring a Client for Weathervane<a name="configuring-client"></a>

#### Overview

Weathervane requires a client system to build the container images and to execute the run harness.

#### Required Software

The following set of software should be installed on the client:
- The Weathervane scripts are written in Perl:
    - Perl 5 (https://www.perl.org/get.html)
- The run harness container will be run on the client using docker:
    - Docker Engine (https://docs.docker.com/install/)
- The Weathervane repository can be cloned using git:
    - Git (https://git-scm.com/downloads)
    - Optionally, the repository can be manually downloaded and extracted.
- The kubeconfig file containing the kubernetes cluster context information must be accessible by this client system.


#### Obtaining Weathervane<a name="obtaining"></a>

The Weathervane repository is located on GitHub.

It can be cloned using git or downloaded and extracted.
Cloning is suggested as it will allow easy updates to newer versions:
1. Clone the Weathervane repository to the client system:
    * `git clone https://github.com/vmware/weathervane`
    * This will create a directory called weathervane in the current directory.
1. Change into the weathervane directory:
    - `cd weathervane`


#### Building and Storing the Weathervane Images<a name="building"></a>

Weathervane uses Docker images for all of its components, including the run 
harness, the workload driver, and all services used by the Weathervane Auction 
application.  These images must be built and pushed to an image repository that 
is accessible to the Kubernetes clusters and the client.

Weathervane includes a script to build the images and push them to
either a Docker Hub account or a private Docker repository.
This script should be run from the weathervane directory.

The process using a Docker Hub account is as follows:
* `./buildDockerImages.pl --username yourUserName`
    * Replace *yourUserName* with your Docker Hub username.
    * The script will prompt for your Docker Hub password.


The process using a private repository is as follows:
* `./buildDockerImages.pl --private --host hostname`
    * Replace *hostname* with the hostname or IP address of the private registry.
    * If your registry is not running on the default port 5000 or requires a username for authentication, these can be specified as follows:
        * `./buildDockerImages.pl --private --host hostname --port 5001 --username yourUserName`
        * The script will prompt for password.

The process to build and push the images can take as long as an hour to complete.

## Using Weathervane<a name="using"></a>

### Overview

This section is a task-oriented guide to using Weathervane.  Each 
section discusses the steps required to achieve a specific goal.  The tasks in 
this chapter all assume that you have performed a run using the instructions in
the [QuickStart](#quickstart-guide) section.

In this section, the term system-under-test (SUT) is used to refer to the 
collection of resources on which the Weathervane application runs.  This may
be an entire Kubernetes cluster, or a set of worker nodes isolated using
[node labels](#nodeLabels).

The tasks discussed in this section are:
- [Change the Number of WvUsers](#task-change)
- [Find the Maximum WvUsers for a Configuration](#task-max)
- [Increase the Load on the SUT: Add Application Instances](#task-moreinstances)
- [Increase the Load on the SUT: Larger Application Instances](#task-largerinstances)
- [Perform an Extended Duration Run](#task-extended)
- [Perform a Series of Runs](#task-series)
- [Cleanly Stop an Ongoing Run](#task-stop)
- [Clean up a Cluster when Finished](#task-clean)

### Change the Number of WvUsers<a name="task-change"></a>

#### Task Goal

This task shows how to change the number of WvUsers used to create load on the
SUT when using the `fixed` run strategy.

The run performed in the [QuickStart](#quickstart-guide) section used the `fixed`
run strategy with the default number of users.  For the micro configuration the 
default is 200 WvUsers.  If the run failed, you may wish to reduce the load on 
the SUT by reducing the number of WvUsers.  If it passed, you may wish to 
increase the number of WvUsers.

#### Instructions

With the `fixed` run strategy, you can change the load on the SUT by specifying
the number of users in your configuration file.  

The number of users specified must be less than or equal to the maximum number 
of users loaded for the selected configuration size.  For the `micro` 
configuration this is `3000` users. For the `small` configuration this is 
`20000` users.

To change the number of users, edit the configuration file as follows:

1. Add the following line: `"users" : 500,`, replacing `500` with the number of 
   users you wish to use.
1. Optionally, change the description parameter to properly describe the run.
1. Optionally, save the configuration file by a different name to reflect the contents.

Then run Weathervane as before with the new configuration file.

### Find the Maximum WvUsers for a Configuration<a name="task-max"></a>

#### Task Goal

This task shows how to configure Weathervane to automatically find the maximum 
WvUsers that can be supported by the configuration described in the configuration
file.

The `fixed` runStrategy used in the [QuickStart](#quickstart-guide) section gives a 
pass/fail result for a fixed number of users.  Finding the maximum number of 
users with the `fixed` run strategy may require a large number 
of runs.  An alternative is to use the `findMaxSingleRun` run strategy.  With 
this run strategy, Weathervane will automatically vary the load in order to 
discover the maximum number of users supported on the SUT by the given 
configuration.  

#### Instructions

To use the `findMaxSingleRun` run strategy, edit the configuration file as 
follows:

1. In the configuration file change the value for the `runStrategy` to 
   `findMaxSingleRun`.
1. Optionally, change the description parameter to properly describe the run.
1. Optionally, save the configuration file by a different name to reflect the contents.

Then run Weathervane as before with the new configuration file.  The output from
the run will be the maximum WvUsers that could run against the deployed 
configuration and still meet the QoS requirements.

### Increase the Load on the SUT: Add Application Instances<a name="task-moreinstances"></a>

#### Task Goal

This task shows how to increase the load on the SUT by adding additional 
instances of the Weathervane Auction application.

In many cases, a running only a single instance of the Weathervane application
will not place sufficient load on the SUT to max out the available resources.
This is because the CPU and memory resources used by an instance of the auction 
application are fixed, and may be less than the resources available on the SUT.
To increase the load on the SUT, you can run multiple copies of the Auction 
application in 
each run. Each copy is referred to as an application instance. Increasing the 
number of application instances will increase the load on the SUT.  The 
number of application instances is limited only by the CPU, memory, and storage 
resources of your cluster. The [Configuration Sizes](#configuration-sizes) section
below discusses the resources used by each configuration size.

When you run the `findMaxSingleRun` run strategy (see [above](#task-max)) with 
multiple application instances, Weathervane will vary the load on 
the instances until all have reached a maximum load.  It will then report the 
maximum passing load for each individual instance and the total load supported 
by the cluster. Increasing the number of application instances until the total 
load ceases to increase will give you the peak capabilities of the cluster.

Note that each time a new application instance is used, the data will need to 
be pre-loaded for the stateful services.  This will increase the time required
for the first run with a new number of application instances.

#### Instructions

To change the number of application instances, edit the configuration file as 
follows:

1. Add the following line: `"numAppInstances" : 2`, 
    - Replace the `2` with the number of instances you wish to run.
1. Optionally, change the description parameter to properly describe the run.
1. Optionally, save the configuration file by a different name to reflect the contents.

Then run Weathervane as before with the new configuration file.

**Note:**

- Each application instance will run in a separate namespace.  For n 
  application instances, the namespaces will be called `auctionw1i1` through 
  `auctionw1in`.  If your platform does not support creating namespaces using 
  kubectl, then you must create these namespaces manually before the run.

### Increase the Load on the SUT: Larger Application Instances<a name="task-largerinstances"></a>

#### Task Goal

This task shows how to increase the load on the SUT by using a larger 
configuration size for your application instances.

Weathervane currently supports two configuration sizes for the Auction 
application: `micro` and `small`. Additional sizes will be added in future releases.
Each size corresponds to a fixed configuration of the Weathervane Auction 
application and an appropriate number of workload driver nodes.  Larger 
configurations will support a large user load, and may come closer to 
maxing out the capabilities of your cluster. A different configuration size 
may also be more representative of production applications to be deployed on 
the SUT.  More detail about the configuration sizes is given in 
the [Configuration Sizes](#configuration-sizes) section below.

Note that the different configuration sizes require different amounts of data
to be pre-loaded, and specify different sizes for the persistent volumes used
by the stateful services.  As a result, changing the configuration size will 
cause existing persistent volumes to be deleted and require the data to be 
reloaded for any previously used instances.

#### Instructions

To use a different configuration size, edit the configuration file as follows:

1. Change the value for the `configurationSize` to `micro` or `small`.
1. Optionally, change the description parameter to properly describe the run.
1. Optionally, save the configuration file by a different name to reflect the contents.

Then run Weathervane as before with the new configuration file.

### Perform an Extended Duration Run<a name="task-extended"></a>

#### Task Goal

This task shows how to run Weathervane for an extended duration. 

The default duration of a run using the `fixed` run strategy is sixteen minutes. 
This consists of a four minute ramp-up period, a five minute warm-up period, a 
single five minute QoS period, and a two minute ramp-down period. Compliance 
with the QoS requirements is checked in the QoS period.

There may times when it is desirable to run Weathervane for an extended 
duration.  For example, you may wish to perform an extended load test of
a cluster, or to observe the effects of changes made to the SUT while the 
Weathervane is running.  You can do this using the `fixed` run strategy by 
adding additional QoS periods. 

#### Instructions

To run using the `fixed` run strategy with a larger number of QoS periods, 
edit the configuration file as follows:

1. Add the following line: `"numQosPeriods" : 2`, 
    - Replace the `2` with the number of QoS periods you wish to use.
    - Each additional QoS period adds five minutes to the run. 
        - For example, to run for 24 hours, or 24*60 = 1440 minutes, 
          you would need 1440/5 = 288 QoS periods.  In this case you would 
          specify the value `288` for `numQosPeriods`. 
1. Optionally, change the description parameter to properly describe the run.
1. Optionally, save the configuration file by a different name to reflect the contents.

Then run Weathervane as before with the new configuration file.

### Perform a Series of Runs<a name="task-series"></a>

#### Task Goal

This task discusses ways to simplify the execution of multiple runs of 
Weathervane.

There are many reasons that you might want to perform multiple successive
runs of Weathervane.  Some examples are:
- Perform multiple runs with a given configuration to assess the variability of
  the results.
- Perform runs with different numbers of application instances to compare 
  clusters at different levels of load.

#### Instructions

The easiest way to perform multiple runs of Weathervane is to use a shell script 
that performs multiple successive invocations. The Weathervane directory 
includes a file called `runmany.sh` which is an example of such a script. When 
running Weathervane from a script, there are two ways to vary the parameter values 
that are used in each run:

1. Use a different configuration file for each run. Each configuration file 
   would have different values for the relevant parameters.
1. Use command-line options for the relevant parameters. Command-line options 
   are values for Weathervane parameters that override the values specified in 
   the configuration file.  Most configuration file parameters can also be 
   specified as command-line options.

Command-line options are specified when running the runWeathervane.pl script.
In order to separate them from the parameters to that script, they must be
specified after the parameters to runWeathervane.pl and separated by two dashes 
(\-\-).  The following is an example of using a command-line option to override  
the run strategy specified in the configuration file:

`./runWeathervane.pl --configFile weathervane.config -- --runStrategy=findMaxSingleRun`

Additional examples are given in the runmany.sh script file.


### Cleanly Stop an Ongoing Run<a name="task-stop"></a>

#### Task Goal

This task discusses how to stop an ongoing run, or clean up after a 
run that was interrupted.

There may be times that you want to stop a run that is in progress.  Simply
stopping the runWeathervane.pl script using ctrl-c does not actually 
stop the run.  The run harness on the client and all components running on 
the Kubernetes clusters will continue to run and place a load on the system. 
Additionally, a run that is interrupted due to a failure on the 
client or the SUT will leave components running on the various systems. As a 
result it is necessary to follow the instructions in this task to cleanly stop 
all components of Weathervane.

Note that starting a new run of Weathervane will also clean up any components 
left running from a previous run.

#### Instructions

To cleanly stop an ongoing run of Weathervane, execute the following command
from a command-line on the client.  The configuration file used should be the 
same as that used in the run to be stopped. 

`./runWeathervane.pl --configFile weathervane.config -- --stop`

### Clean up a Cluster when Finished<a name="task-clean"></a>

#### Task Goal

This task describes how to return your Kubernetes clusters to their original
state after running Weathervane.

In order to avoid reloading data for each run, Weathervane leaves the following 
constructs in place on the Kubernetes clusters at the end of a run:
- Namespaces
- Persistent Volume Claims (PVCs)
- Persistent Volumes (PVs)

When you are done using Weathervane you should delete these constructs.  This 
is particularly important for the PVCs. If they are not deleted the 
associated PVs will not be removed, and will continue to take up space on the
underlying physical storage.  You should delete the PVCs even if you are 
planning to tear down the Kubernetes cluster, as a failure to delete the PVCs
may leave inaccessible files on the underlying storage.

#### Instructions

You can clean up by running the following commands against all Kubernetes
clusters used in your tests.  

Delete the PVCs:

Deleting the PVCs will cause the associated PVs to be deleted.

`kubectl delete pvc --selector=app=auction --all-namespaces --all`

Delete the namespaces:

If you manually created the namespaces, then you will have to manually delete 
them using the appropriate method for your cluster. Otherwise, you can use
the following command,

`kubectl delete ns --selector=app=auction --all`

If you have multiple kubeconfig files for different clusters, you will need to 
include the `--kubeconfig=` parameter in these commands.

If you have multiple contexts for different clusters defined in a kubeconfig 
file, then you will need to include the `--context=` parameter in these commands.

## Configuration File Details<a name="config"></a>

The Weathervane configuration file controls all options for a Weathervane run.

Example Weathervane configuration files can be found at `weathervane/weathervane.config.k8s.micro` and `weathervane/weathervane.config.k8s.small`.

When you start a run, the configuration file must be specified as an argument to the `runWeathervane.pl` script:

`./runWeathervane.pl --configFile=weathervane.config.k8s.small`

This section discusses the different configuration file parameters. See [Using Weathervane](#using) for a hands-on introduction to the Weathervane configuration file.

### JSON Primer<a name="JSONprimer"></a>

The Weathervane configuration file uses an extended JSON format.  Quoting from
the json.org web site: “JSON (JavaScript Object Notation) is a lightweight
data-interchange format. It is easy for humans to read and write. It is easy for
machines to parse and generate.”

The basic construct in JSON is a JSON object, which is represented as a set of
key-value pairs enclosed in curly braces.  The entire Weathervane configuration
file is a single JSON object.  All keys in a JSON object are strings, which must
be enclosed in double quotes.

Values in JSON may be strings enclosed in double quotes, numbers, the Boolean
values true and false (no double quotes), a JSON object, a JSON array, or the
null value.  A JSON array is a sequence of JSON values surrounded by square
brackets and separated by commas. The null value is not used in the Weathervane
configuration file.

***Please note all keys and values in the configuration file are case sensitive.***

The Weathervane run harness allows two extensions to the JSON standard to be
used in the configuration file:

* The file may contain comments, which start with the # symbol. All comments are
  ignored when parsing the file.
* The last key-value pair in a JSON object, and the last value in a JSON array,
  may be followed by a comma.  This simplifies making changes to the file as you
  can delete or comment out lines and not worry about needing to remove the
  comma on the previous line.

**Example Weathervane configuration file**

```json
{
"description" : "Initial Test Run",
"configurationSize": "micro",
"runStrategy" : "fixed",
"numAppInstances" : 2,
"dockerNamespace" : "yourRepository",
"kubernetesClusters" : [ 
  { 
    "name" : "appCluster", 
    "kubeconfigFile" : "/root/.kube/config",
    "kubeconfigContext" : "cluster-context-1",
    "useLoadBalancer" : true,
  },
  { 
    "name" : "driverCluster", 
    "kubeconfigFile" : "/root/.kube/config",
    "kubeconfigContext" : "cluster-context-2",
    "useLoadBalancer" : true,
  },
],

  "driverCluster" : "driverCluster",
  "appInstanceCluster" : "appCluster",
  "cassandraDataStorageClass" : "weathervanesc",
  "postgresqlStorageClass" : "weathervanesc",
  "nginxCacheStorageClass" : "weathervanesc",
}
```

### Required Configuration Options

This section describes configuration options required to run Weathervane.

#### dockerNamespace

The `dockerNamespace` parameter tells Weathervane where to find the Docker Images that you built using the `buildDockerImages.pl` script.

| Configuration Parameter: dockerNamespace |
| ---------------------------------------- |
| `"dockerNamespace" : "yourRepository",`  |

where `yourRepository` is your Docker Hub username or the hostname and port of your private Docker registry.

#### Specifying Kubernetes Clusters

The `kubernetesClusters` block describes the Kubernetes cluster(s) on which Weathervane runs. This is a 
JSON list of JSON objects, where each object represents a Kubernetes cluster. At least one Kubernetes cluster must be specified, and the parameters `name`, `kubeconfigFile`, `kubeconfigContext` and `useLoadBalancer` must be specified for each cluster.  See [Configuring Kubernetes Cluster(s) for Weathervane](#configuring-clusters) for details.

##### name<a name="kubernetesClusters-name"></a>

The `name` parameter is the name you choose for each cluster and is used to select the driver and Auction application clusters using the `driverCluster` and `appInstanceCluster` parameters in the configuration file.

| Configuration Parameter: name |
| ---------------------------------------- |
| `"name" : "driverCluster",`  |

where `driverCluster` must match a value from the section [Selecting Kubernetes Clusters](#selecting-clusters).

##### kubeconfigFile

The kubeconfigFile parameter specifies the *kubeconfig* file which is typically used to configure access to Kubernetes clusters. See [Kubernetes Cluster Credentials](#credentials) for details.

| Configuration Parameter: kubeconfigFile    |
| ------------------------------------------ |
| `"kubeconfigFile" : "/root/.kube/config",` |

where `/root/.kube/config` is the path and filename of your *kubeconfig* file on the Weathervane client.

##### kubeconfigContext

The `kubeconfigContext` parameter specifies the name of the context for your cluster such as `kubernetes-admin@kubernetes`. This can be found in your *kubeconfig* file. Set an empty string (`""`) to use the `current-context` defined in the `kubeconfigFile`.

| Configuration Parameter: kubeconfigContext             |
| ------------------------------------------------------ |
| `"kubeconfigContext" : "kubernetes-admin@kubernetes",` |

where `kubernetes-admin@kubernetes` is the name of your cluster's context. 

##### useLoadBalancer

The `useLoadBalancer` parameter specifies whether application instances use a NodePort or LoadBalancer service to enable incoming communication from the workload driver pods. See [NodePort and LoadBalancer Services](#loadbalancer) for details.

| Configuration Parameter: useLoadBalancer                     |
| ------------------------------------------------------------ |
| `"useLoadBalancer" : true,`<BR>or<BR>`"useLoadBalancer" : false,` |

#### Selecting Kubernetes Clusters<a name="selecting-clusters"></a>

The `driverCluster` and `appInstanceCluster` parameters specify which clusters in `kubernetesClusters` should be used to host workload driver pods and Auction application pods respectively.

| Configuration Parameter: driverCluster |
| -------------------------------------- |
| `"driverCluster" : "driverCluster",`   |

| Configuration Parameter: appInstanceCluster |
| ------------------------------------------- |
| `"appInstanceCluster" : "appCluster",`      |

where `driverCluster` and `appCluster` are the names of the workload driver cluster and Auction application cluster respectively. These names should be one of the `name` values specified in `kubernetesClusters`. See the [name configuration parameter](#kubernetesClusters-name).

#### Storage Classes

The `cassandraDataStorageClass`, `postgresqlStorageClass`, and `nginxCacheStorageClass` parameters specify which Kubernetes *StorageClass* should be used for persistent storage. All services can use the same StorageClass, or you can use a different StorageClass for each service. See [Configuring Persistent Storage](#storageclass) for details.

| Configuration Parameter: Storage Classes                     |
| ------------------------------------------------------------ |
| `"cassandraDataStorageClass" : "weathervanesc",`<BR>`"postgresqlStorageClass" : "weathervanesc",`<BR> `"nginxCacheStorageClass" : "weathervanesc",` |

where you should replace `weathervanesc` with the name of your StorageClass.

### Common Configuration Options

This section describes the most commonly used configuration options for Weathervane.

#### Run Description

The `description` parameter lets you save a description of the run in its `weathervaneResults.csv` summary. This description will be saved in the run's line in the `description` column. See [Run Output](#output) for more details.

| Configuration Parameter: description  |
| ------------------------------------- |
| `“description” : “Initial Test Run”,` |

where `Initial Test Run` is any string you specify. 

#### Run Strategies

Weathervane has two types of run strategies. You can select a run strategy using the configuration parameter `runStrategy`.

##### fixed

A fixed run gives a pass or fail result for a fixed number of users. A fixed run takes approximately 20 to 30 minutes to complete.

| Configuration Parameter: fixed Run Strategy |
|-------------------------------|
| `"runStrategy" : "fixed",` |

##### findMaxSingleRun

A run using findMaxSingleRun Run Strategy will automatically vary the load in order to discover the maximum number of users supported by the given configuration size on the SUT.

The outcome of a findMaxSingleRun is the Maximum WvUsers that pass at QoS. The run length of a findMaxSingleRun varies, but typically takes between 1.5 to 2 hours to complete. 

You can use findMaxSingleRun with multiple application instances to scale up the load on the SUT.

| Configuration Parameter: findMaxSingleRun Run Strategy |
|-------------------------------|
| `"runStrategy" : "findMaxSingleRun",` |

#### Configuration Sizes<a name="configuration-sizes"></a>

Weathervane supports two configuration sizes: `micro` and `small`. Each size corresponds to a fixed configuration of the Weathervane Auction application and an appropriate number of workload driver nodes.  The `small` configuration size supports a larger user load than the `micro` configuration size, and may come closer to maxing out the capabilities of your cluster. A different configuration size may also be more representative of production applications to be deployed on the cluster under test.  

You can select a configuration size using the parameter `configurationSize`.

| Configuration Parameter: Micro Configuration Size |
|-------------------------------|
| `"configurationSize": "micro",` |

A micro application instance can support roughly up to 1000 users.

| Configuration Parameter: Small Configuration Size |
|-------------------------------|
| `"configurationSize": "small",` |

A small application instance can support roughly up to 10,000 users.

**Table: User Defaults for micro and small Configuration Sizes**

| Configuration Size          | micro | small |
| --------------------------- | ----- | ----- |
| Default users for fixed run | 200   | 2000  |
| Default maximum users supported | 3000   | 20,000  |


Table 1 below shows the total CPU and memory resources requested by the application and driver pods for each configuration size.  These request levels are per application instance.  Table 2 provides a quick reference for the total application pod resources required at different numbers of application instances.  You can use these tables to find the maximum number of application instances you will be able to deploy on your clusters.

**Table 1: Resource Requirements For Each Configuration Size, One Application Instance**
<table border=0 cellpadding=0 cellspacing=0 width=546 style='border-collapse:
 collapse;table-layout:fixed;width:410pt'>
 <col class=xl65 width=128 style='mso-width-source:userset;mso-width-alt:4096;
 width:96pt'>
 <col class=xl65 width=37 style='mso-width-source:userset;mso-width-alt:1194;
 width:28pt'>
 <col class=xl65 width=101 style='mso-width-source:userset;mso-width-alt:3242;
 width:76pt'>
 <col class=xl65 width=71 style='mso-width-source:userset;mso-width-alt:2261;
 width:53pt'>
 <col class=xl65 width=37 style='mso-width-source:userset;mso-width-alt:1194;
 width:28pt'>
 <col class=xl65 width=101 style='mso-width-source:userset;mso-width-alt:3242;
 width:76pt'>
 <col class=xl65 width=71 style='mso-width-source:userset;mso-width-alt:2261;
 width:53pt'>
 <tr height=21 style='height:16.0pt'>
  <td height=21 class=xl65 width=128 style='height:16.0pt;width:96pt'>Configuration
  Size</td>
  <td colspan=3 class=xl66 width=209 style='text-align: center;border-right:1.0pt solid black;
  width:157pt'><b>micro</b></td>
  <td colspan=3 class=xl66 width=209 style='text-align: center;border-right:1.0pt solid black;
  border-left:none;width:157pt'><b>small</b></td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 class=xl65 style='height:16.0pt'></td>
  <td class=xl69>CPU</td>
  <td class=xl70>Memory (GiB)</td>
  <td class=xl71>Disk (GiB)</td>
  <td class=xl69 style='border-left:none'>CPU</td>
  <td class=xl70>Memory (GiB)</td>
  <td class=xl71>Disk (GiB)</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 class=xl65 style='height:16.0pt'>Total Driver</td>
  <td class=xl72 align=right>0.50</td>
  <td class=xl65 align=right>1.66</td>
  <td class=xl73 align=right>0.00</td>
  <td class=xl72 align=right style='border-left:none'>3.30</td>
  <td class=xl65 align=right>19.04</td>
  <td class=xl73 align=right>0.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 class=xl65 style='height:16.0pt'>Total App</td>
  <td class=xl72 align=right>1.21</td>
  <td class=xl65 align=right>5.57</td>
  <td class=xl73 align=right>32.00</td>
  <td class=xl72 align=right style='border-left:none'>6.00</td>
  <td class=xl65 align=right>53.42</td>
  <td class=xl73 align=right>150.00</td>
 </tr>
 <tr height=23 style='height:17.0pt'>
  <td height=23 class=xl74 style='height:17.0pt'>Total</td>
  <td class=xl75 align=right style='border-left:none'>1.71</td>
  <td class=xl76 align=right>7.23</td>
  <td class=xl77 align=right>32.00</td>
  <td class=xl75 align=right style='border-left:none'>9.30</td>
  <td class=xl76 align=right>72.46</td>
  <td class=xl77 align=right>150.00</td>
 </tr>
</table>

**Table 2: Application Resource Requirements For Each Configuration Size, Multiple Application Instances**
<table border=0 cellpadding=0 cellspacing=0 width=547 style='border-collapse:
 collapse;table-layout:fixed;width:410pt'>
 <col width=93 style='mso-width-source:userset;mso-width-alt:2986;width:70pt'>
 <col width=45 style='mso-width-source:userset;mso-width-alt:1450;width:34pt'>
 <col width=103 style='mso-width-source:userset;mso-width-alt:3285;width:77pt'>
 <col width=75 style='mso-width-source:userset;mso-width-alt:2389;width:56pt'>
 <col width=53 style='mso-width-source:userset;mso-width-alt:1706;width:40pt'>
 <col width=103 style='mso-width-source:userset;mso-width-alt:3285;width:77pt'>
 <col width=75 style='mso-width-source:userset;mso-width-alt:2389;width:56pt'>
 <tr height=21 style='height:16.0pt'>
  <td height=21 width=93 style='height:16.0pt;width:70pt'>Configuration Size</td>
  <td colspan=3 class=xl65 width=223 style='text-align: center;width:167pt'><b>micro</b></td>
  <td colspan=3 class=xl65 width=231 style='text-align: center;width:173pt'><b>small</b></td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 style='height:16.0pt'>Number of Application Instances</td>
  <td>CPU</td>
  <td>Memory (GiB)</td>
  <td>Disk (GiB)</td>
  <td>CPU</td>
  <td>Memory (GiB)</td>
  <td>Disk (GiB)</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>1</td>
  <td class=xl66 align=right>1.21</td>
  <td class=xl66 align=right>5.57</td>
  <td class=xl66 align=right>32.00</td>
  <td class=xl66 align=right>6.00</td>
  <td class=xl66 align=right>53.42</td>
  <td class=xl66 align=right>150.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>2</td>
  <td class=xl66 align=right>2.42</td>
  <td class=xl66 align=right>11.13</td>
  <td class=xl66 align=right>64.00</td>
  <td class=xl66 align=right>12.00</td>
  <td class=xl66 align=right>106.84</td>
  <td class=xl66 align=right>300.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>3</td>
  <td class=xl66 align=right>3.63</td>
  <td class=xl66 align=right>16.70</td>
  <td class=xl66 align=right>96.00</td>
  <td class=xl66 align=right>18.00</td>
  <td class=xl66 align=right>160.25</td>
  <td class=xl66 align=right>450.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>4</td>
  <td class=xl66 align=right>4.84</td>
  <td class=xl66 align=right>22.27</td>
  <td class=xl66 align=right>128.00</td>
  <td class=xl66 align=right>24.00</td>
  <td class=xl66 align=right>213.67</td>
  <td class=xl66 align=right>600.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>5</td>
  <td class=xl66 align=right>6.06</td>
  <td class=xl66 align=right>27.83</td>
  <td class=xl66 align=right>160.00</td>
  <td class=xl66 align=right>30.00</td>
  <td class=xl66 align=right>267.09</td>
  <td class=xl66 align=right>750.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>6</td>
  <td class=xl66 align=right>7.27</td>
  <td class=xl66 align=right>33.40</td>
  <td class=xl66 align=right>192.00</td>
  <td class=xl66 align=right>36.00</td>
  <td class=xl66 align=right>320.51</td>
  <td class=xl66 align=right>900.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>7</td>
  <td class=xl66 align=right>8.48</td>
  <td class=xl66 align=right>38.96</td>
  <td class=xl66 align=right>224.00</td>
  <td class=xl66 align=right>42.00</td>
  <td class=xl66 align=right>373.93</td>
  <td class=xl66 align=right>1050.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>8</td>
  <td class=xl66 align=right>9.69</td>
  <td class=xl66 align=right>44.53</td>
  <td class=xl66 align=right>256.00</td>
  <td class=xl66 align=right>48.00</td>
  <td class=xl66 align=right>427.34</td>
  <td class=xl66 align=right>1200.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>9</td>
  <td class=xl66 align=right>10.90</td>
  <td class=xl66 align=right>50.10</td>
  <td class=xl66 align=right>288.00</td>
  <td class=xl66 align=right>54.00</td>
  <td class=xl66 align=right>480.76</td>
  <td class=xl66 align=right>1350.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>10</td>
  <td class=xl66 align=right>12.11</td>
  <td class=xl66 align=right>55.66</td>
  <td class=xl66 align=right>320.00</td>
  <td class=xl66 align=right>60.00</td>
  <td class=xl66 align=right>534.18</td>
  <td class=xl66 align=right>1500.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>11</td>
  <td class=xl66 align=right>13.32</td>
  <td class=xl66 align=right>61.23</td>
  <td class=xl66 align=right>352.00</td>
  <td class=xl66 align=right>66.00</td>
  <td class=xl66 align=right>587.60</td>
  <td class=xl66 align=right>1650.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>12</td>
  <td class=xl66 align=right>14.53</td>
  <td class=xl66 align=right>66.80</td>
  <td class=xl66 align=right>384.00</td>
  <td class=xl66 align=right>72.00</td>
  <td class=xl66 align=right>641.02</td>
  <td class=xl66 align=right>1800.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>13</td>
  <td class=xl66 align=right>15.74</td>
  <td class=xl66 align=right>72.36</td>
  <td class=xl66 align=right>416.00</td>
  <td class=xl66 align=right>78.00</td>
  <td class=xl66 align=right>694.43</td>
  <td class=xl66 align=right>1950.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>14</td>
  <td class=xl66 align=right>16.95</td>
  <td class=xl66 align=right>77.93</td>
  <td class=xl66 align=right>448.00</td>
  <td class=xl66 align=right>84.00</td>
  <td class=xl66 align=right>747.85</td>
  <td class=xl66 align=right>2100.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>15</td>
  <td class=xl66 align=right>18.17</td>
  <td class=xl66 align=right>83.50</td>
  <td class=xl66 align=right>480.00</td>
  <td class=xl66 align=right>90.00</td>
  <td class=xl66 align=right>801.27</td>
  <td class=xl66 align=right>2250.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>16</td>
  <td class=xl66 align=right>19.38</td>
  <td class=xl66 align=right>89.06</td>
  <td class=xl66 align=right>512.00</td>
  <td class=xl66 align=right>96.00</td>
  <td class=xl66 align=right>854.69</td>
  <td class=xl66 align=right>2400.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>17</td>
  <td class=xl66 align=right>20.59</td>
  <td class=xl66 align=right>94.63</td>
  <td class=xl66 align=right>544.00</td>
  <td class=xl66 align=right>102.00</td>
  <td class=xl66 align=right>908.11</td>
  <td class=xl66 align=right>2550.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>18</td>
  <td class=xl66 align=right>21.80</td>
  <td class=xl66 align=right>100.20</td>
  <td class=xl66 align=right>576.00</td>
  <td class=xl66 align=right>108.00</td>
  <td class=xl66 align=right>961.52</td>
  <td class=xl66 align=right>2700.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>19</td>
  <td class=xl66 align=right>23.01</td>
  <td class=xl66 align=right>105.76</td>
  <td class=xl66 align=right>608.00</td>
  <td class=xl66 align=right>114.00</td>
  <td class=xl66 align=right>1014.94</td>
  <td class=xl66 align=right>2850.00</td>
 </tr>
 <tr height=21 style='height:16.0pt'>
  <td height=21 align=right style='height:16.0pt'>20</td>
  <td class=xl66 align=right>24.22</td>
  <td class=xl66 align=right>111.33</td>
  <td class=xl66 align=right>640.00</td>
  <td class=xl66 align=right>120.00</td>
  <td class=xl66 align=right>1068.36</td>
  <td class=xl66 align=right>3000.00</td>
 </tr>
</table>

*These figures do not include resources being used by the Kubernetes system pods.*

*Information on resources being used by system pods is available using `kubectl describe node` in the 'allocated resources' section.*

#### Multiple Application Instances

Weathervane can run multiple copies of the Auction application in each run. Each copy is referred to as an `application instance`. Increasing the number of application instances will increase the load on the cluster under test. The number of application instances is limited only by the CPU, memory, and storage resources of your cluster. You can select the number of application instances using the parameter `numAppInstances`.


| Configuration Parameter: Application Instance |
| --------------------------------------------- |
| `"numAppInstances" : N,`                      |

Where `N` is the number of application instances you want to run.

#### Default Common Configuration Values

When a run parameter is not specified in the configuration file, Weathervane uses the default value. 
Configuration parameters not mentioned here have an empty string as the default value. 

| Configuration Parameter   | Default Value |
|---------------------------|-----------------------------|
| `configurationSize`        | `micro`                       |
| `runStrategy`               | `findMaxSingleRun`            |
| `numAppInstances`           | `1`                           |
| `kubeconfigFile`            | `~/.kube/config`              |
| `useLoadBalancer`           | `TRUE`                        |
| `cassandraDataStorageClass` | `weathervanesc`               |
| `posgresqlStorageClass`     | `weathervanesc`               |
| `nginxCacheStorageClass`    | `weathervanesc`               |

## Run Output Details<a name="output"></a>

### Overview

While a run is in progress, Weathervane will print output to the console about the
current activity and the state of the run.  It will also create files in an output
directory associated with the run.  In this section we will discuss both types
of run output.

### Console Output

When you start a run of Weathervane, the run harness will print information 
about configuration to be used in the run, and then will provide messages
as it starts each phase.  These phases are:
- Clean-up, which stops any running services left over from a previous run.
- Start data services, which starts all stateful services that need to be loaded 
  with data before the run.
- Prepare data, which restores data loaded for a previous run to its original state.
- Load Data, which loads the data for any data services not loaded on a previous run.
- Start back-end services
- Start front-end services
- Start run, which includes starting all workload driver nodes

Once all of the workload driver nodes have started, the run harness will start the run 
strategy.  The exact messages provided will depend on the selected run strategy and the 
number of application instances.  The output will include:
- A message for each ramp interval with the number of WvUsers per instance.
- A message for each warmup interval with the number of WvUsers per instance.
- For each application instance, a message for each QoS interval indicating whether
  it passed or failed the QoS requirements.  If the instance 
  failed QoS, then the output will include a list of the requirements which 
  were not met. 

After the run strategy has completed, the run harness will repeat the clean-up phase 
to remove any running services. It will then print the run result for each application
instance, as well as the result for the overall run.

### Output Files

When Weathervane is run, a new directory is created that contains output files 
created during the run.  These files contain a variety of useful information about 
the configuration and results of each run.  They also include files that will be 
useful in debugging failing runs.  By default, these directories are created in a 
directory called _output_ that is located in the directory where you start 
`runWeathervane.pl`. A different output directory can be specified using the 
`--output` option to the script. The output directories are discussed in more detail 
in the next section.

In addition to the per-run directories, the run harness also creates a file called 
`weathervaneResults.csv` in the directory where you start 
`runWeathervane.pl`. At the end of each run, the run harness will write two new lines 
to this file: a header line and a data line.  The data line will contain a summary 
of the data from the run.  This file simplifies the task of comparing results from 
multiple runs. 

#### Individual Run Files

Each time Weathervane is run, the run is assigned a unique run number.  The output 
for that run is then placed in the directory `output/n`, where n is the run number.

The key in this directory are:
- **`console.log`**: This contains the same output that appeared on the console during 
  the run.
-  **`0/`**: This is a subdirectory which contains more detailed files related to the 
   run.  Future run strategies may include multiple sub-runs for a single invocation 
   of Weathervane.  Those additional sub-runs will have their run details under 
   directories **`1/`**, **`2/`**, etc.  The files in this directory are discussed 
   below.
- **`debug.log`**: This file contains detailed debug output from the run harness, and 
  should be included when creating an issue on the Weathervane repository.
- **`version.txt`**: The version number of Weathervane that was used in the run.
- **`*.save`**: These are files that capture the exact set of parameters used when 
  running Weathervane.  They can be used to recreate the run.

The most important information under the **`0/`** directory is:
- **`EndRunReport-appInstance*.json`**: There will be one of these files for each application 
  instance.  It contains a summary of the operation statistics for the QoS period that 
  passed with the largest number of WvUsers.
- **`run-W1.log`**: This file contains the periodic output from the Weathervane workload 
  controller giving the statistics for the application instances at 30 second intervals. 
  Similar information is available in csv form under the **`/0/statistics/workloadDriver/`** 
  directory.
- **`configuration/`**: This directory contains a significant amount of information 
  about the run-time configuration of the Weathervane components and the Kubernetes 
  clusters.  For example, in the 
  **`configuration/clusters/clusterName/`**  directory for each Kubernetes cluster 
     there will be `kubectl get` 
  and `kubectl describe` output that shows the placement of pods and the status 
  of the nodes during the run. 

Most of the other files and directories in the **`0/`** directory contain information 
that will be only useful when debugging issues, and may be requested if you file an issue 
on the Weathervane repository.

## Troubleshooting<a name="troubleshooting"></a>

### Overview

This section contains troubleshooting information for a number of issues that may be encountered with Weathervane.

#### Insufficient Resources and FailedScheduling

When all of the resources (cpu and memory) of the Kubernetes cluster have been requested,
the Weathervane runHarness will encounter failures when attempting to start additional pods.
This can happen when you attempt to deploy more application instances than can fit on the available worker nodes.
See the configuration size [tables](#configuration-sizes) for information on the resources required by each instance.
You should also check whether cluster resources have been requested by non-Weathervane pods.

Below is some example output that might occur during such a scenario:
* > Error running kubernetes pod : FailedScheduling podLabelString type=webServer, namespace auctionw1i12  
Couldn't bring to running all frontend services for appInstance 12 of workload 1.

More specific details about resource information can be obtained using the 'kubectl describe' commands.

If resources cannot be added or freed, then the Weathervane runs will need to use fewer application instances or a smaller configuration size.

#### Data Services Issues

On rare occasions, the data constructs stored on the persistent storage may become corrupted and prevent the data services from starting.
The following example output might occur for such a scenario:
* > Couldn't bring to running all data services for appInstance 12 of workload 1.

To clear up any data services issues, it is recommended to delete the PersistentVolumeClaim for the affected instance (instance 12 in this example):
* `kubectl delete pvc --selector=app=auction --namespace=auctionw1i12`

The data will be reloaded on the next run with this instance.

## Advanced Topics<a name="advanced"></a>

### Overview

This section contains instructions for handling situations that may not apply to 
all users.

### Running Weathervane on Kubernetes Clusters with Credentials that Expire<a name="expire"></a>

Weathervane relies on the credentials in the kubeconfig file in order to perform
operations on the Kubernetes clusters.  For many cluster providers those 
credentials are valid for only a limited time before needing to be refreshed. 
In order to successfully complete a run of Weathervane on such clusters, it is 
necessary to occasionally refresh the credentials. The runWeathervane.pl script 
provides support for automating the refresh process.

The process for automating the credentials refresh process is as follows:

1. Create an executable script that contains the commands necessary to refresh 
the credentials for your cluster.
    - Check the documentation for your cluster provider for the required commands.
    - For many cluster providers, refreshing the credentials will require logging
    into an API using provider-specific tools. In those cases you will need install
    any required tools on your client.
2. Determine how often this script needs to be run to keep the credentials valid.
3. You then use the `--script` and `--scriptPeriod` parameters to runWeathervane.pl 
to have the script executed every `scriptPeriod` seconds during the run.

For example, consider running Weathervane on a cluster created on Google's GKE
environment. To manage the GKE cluster, you must install the *gcloud* tool from 
the Google Cloud SDK on your client and use it login to GKE.  You also use the
gcloud tool to generate a kubeconfig entry for your cluster in your kubeconfig 
file.  However, the credentials in that entry expire in a time shorter than
a typical Weathervane run. To run Weathervane on this cluster, we must therefore
use a script to refresh the credentials.

In the case of GKE, all that is required to refresh the credentials is to execute 
a kubectl command, assuming that you have logged into gcloud.  As a result,
executing the following script on the client will be sufficient:

```
#!/usr/bin/bash
kubectl get pod --all-namespaces
```

Assuming that we saved this script to a file named *refreshGkeCred.sh*, made it
executable (`chmod +x refreshGkeCred.sh`), and want 
to run it every 5 minutes (300 seconds) during the run, we would start a run of 
Weathervane using the following command:

`./runWeathervane.pl --configFile weathervane.config.gke --script refreshGkeCred.sh --scriptPeriod 300`

### Partitioning Worker Nodes with Labels<a name="nodeLabels"></a>

When running Weathervane on a single Kubernetes cluster, it is desirable to 
separate the workload driver pods and the application pods onto different 
worker nodes.  This isolates the two portions of the benchmark and can give
results that are easier to compare across clusters or cluster configurations. 
If you know the mapping of worker nodes to physical servers,
you can get better isolation by assigning the driver and application 
pods to worker nodes running on different servers. This information
may not be available on cloud-based clusters.

Weathervane supports separating the driver and application pods onto 
different worker nodes using Kubernetes node labels.  You do this as follows:

1. Label the worker nodes you want to use for the workload driver pods with the
label `wvrole=driver`.
    - For example: `kubectl label node worker-node-1 wvrole=driver`
1. Label the worker nodes you want to use for the application (SUT) pods with the
label `wvrole=sut`.
    - For example: `kubectl label node worker-node-2 wvrole=sut`

Important things to know about using these node labels:
- Workload driver pods will never run on worker nodes with the label `wvrole=sut`.
- Application pods will never run on worker nodes with the label `wvrole=driver`.
- Workload driver pods will prefer to run on worker nodes with the label 
`wvrole=driver`, but they will run on nodes without this label if the Kubernetes
scheduler decides that is the best placement.
- Application pods will prefer to run on worker nodes with the label 
`wvrole=sut`, but they will run on nodes without this label if the Kubernetes
scheduler decides that is the best placement.

As a result, when using node labels it is recommended that you label all of your 
worker nodes with either `wvrole=driver` or `wvrole=sut`.  This will give you the 
best control over pod placement.


